{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sought-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "\n",
    "from teeport import Teeport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-emission",
   "metadata": {},
   "source": [
    "## Train a RL model to tune the BTS correctors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-hotel",
   "metadata": {},
   "source": [
    "### Grab the BTS tracking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divided-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "teeport = Teeport('ws://lambda-sp3:8090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "insured-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = teeport.use_processor('6La3OgFJq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-thread",
   "metadata": {},
   "source": [
    "### Define the BTS env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "middle-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "representative-uncle",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BTSEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    This is an env where the agent must learn to tune the correctors\n",
    "    to get the maximum injection efficien. \n",
    "    \"\"\"\n",
    "    metadata = {'render.modes': ['console']}\n",
    "    # Define constants for clearer code\n",
    "    H1_UP = 0\n",
    "    H1_DOWN = 1\n",
    "    V1_UP = 2\n",
    "    V1_DOWN = 3\n",
    "    H5_UP = 4\n",
    "    H5_DOWN = 5\n",
    "    V2_UP = 6\n",
    "    V2_DOWN = 7\n",
    "\n",
    "    def __init__(self, process, knob_step_size=0.1, max_steps=10000, verbose=False):\n",
    "        super(BTSEnv, self).__init__()\n",
    "        \n",
    "        # The trackBTS processor\n",
    "        self.process = process\n",
    "        self.knob_step_size = knob_step_size\n",
    "        self.max_steps = max_steps\n",
    "        self.step_count = 0\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        # Initialize the agent at the middle position of the knobs\n",
    "        # and the zero-offset point of dX\n",
    "        self.knob_pos = 0.5 * np.ones(4)\n",
    "        self.dX = 0.5 * np.ones(6)\n",
    "        \n",
    "        self.inj_eff = None\n",
    "\n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects\n",
    "        # We have 4 knobs, each knob has two actions: go up and go down by 1 unit\n",
    "        # 0: h1 + 0.1, 1: h1 - 0.1\n",
    "        # 2: v1 + 0.1, 3: v1 - 0.1\n",
    "        # 4: h5 + 0.1, 5: h5 - 0.1\n",
    "        # 6: v2 + 0.1, 7: v2 - 0.1\n",
    "        self.action_space = spaces.Discrete(8)\n",
    "        # The observation will be the readout of the BPMs, plus the position of the 4 knobs\n",
    "        # this can be described both by Discrete and Box space\n",
    "        low = np.array([-100] * 18 + [0] * 4)\n",
    "        high = np.array([100] * 18 + [1] * 4)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(22,), dtype=np.float32)\n",
    "    \n",
    "    def get_reward(self, quiet=False):\n",
    "        X = 0.5 * np.ones(22)\n",
    "        X[:6] = self.dX\n",
    "        X[6] = self.knob_pos[0]  # h1\n",
    "        X[10] = self.knob_pos[2]  # h5\n",
    "        X[15] = self.knob_pos[1]  # v1\n",
    "        X[16] = self.knob_pos[3]  # v2\n",
    "        X = X.reshape(1, -1)  # 1D to 2D\n",
    "        \n",
    "        Y = self.process(X)  # TODO: make the returned value always a 2D array\n",
    "        self.inj_eff = Y[0]  # update the injection efficiency for monitoring the process\n",
    "        if (not quiet) and self.verbose:\n",
    "            self.render()\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Important: the observation must be a numpy array\n",
    "        :return: (np.array) \n",
    "        \"\"\"\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Initialize the agent at the middle position of the knobs\n",
    "        # and a x, xp, y, yp randomly offset dX\n",
    "        self.knob_pos = 0.5 * np.ones(4)\n",
    "        \n",
    "        self.dX = 0.5 * np.ones(6)\n",
    "        offset = 0.2 * np.random.rand(4) - 0.1\n",
    "        self.dX[:4] += offset\n",
    "        \n",
    "        if self.verbose:\n",
    "            print('\\n' + '-' * 80)\n",
    "            print(f'dX: {self.dX[0]:.4f}, {self.dX[1]:.4f}, {self.dX[2]:.4f}, {self.dX[3]:.4f}\\n')\n",
    "        \n",
    "        # Calculate the reward and other states\n",
    "        Y = self.get_reward(quiet=True)  # inj_eff, bpm1_x, bpm1_y, bpm2_x, ...\n",
    "        \n",
    "        obs = np.hstack((Y[1:], self.knob_pos))\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == self.H1_UP:\n",
    "            self.knob_pos[0] += self.knob_step_size\n",
    "        elif action == self.H1_DOWN:\n",
    "            self.knob_pos[0] -= self.knob_step_size\n",
    "        elif action == self.V1_UP:\n",
    "            self.knob_pos[1] += self.knob_step_size\n",
    "        elif action == self.V1_DOWN:\n",
    "            self.knob_pos[1] -= self.knob_step_size\n",
    "        elif action == self.H5_UP:\n",
    "            self.knob_pos[2] += self.knob_step_size\n",
    "        elif action == self.H5_DOWN:\n",
    "            self.knob_pos[2] -= self.knob_step_size\n",
    "        elif action == self.V2_UP:\n",
    "            self.knob_pos[3] += self.knob_step_size\n",
    "        elif action == self.V2_DOWN:\n",
    "            self.knob_pos[3] -= self.knob_step_size\n",
    "        else:\n",
    "            raise ValueError(f'Received invalid action={action} which is not part of the action space')\n",
    "            \n",
    "        self.step_count += 1\n",
    "\n",
    "        # Account for the knob position boundaries\n",
    "        self.knob_pos = np.clip(self.knob_pos, 0, 1)\n",
    "        Y = self.get_reward()\n",
    "\n",
    "        obs = np.hstack((Y[1:], self.knob_pos))\n",
    "        reward = Y[0]\n",
    "        \n",
    "        # Do we have a high enough injection efficiency already?\n",
    "        done = bool((reward > 0.86) or (self.step_count == self.max_steps))\n",
    "\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        if mode != 'console':\n",
    "            pass\n",
    "#             raise NotImplementedError()\n",
    "        # Just print out the knob pos and the injection efficiency\n",
    "#         print(f'dX: {self.dX[0]:.4f}, {self.dX[1]:.4f}, {self.dX[2]:.4f}, {self.dX[3]:.4f}')\n",
    "        print(f'h1: {self.knob_pos[0]:.2f}, v1: {self.knob_pos[1]:.2f}, ' + \\\n",
    "              f'h5: {self.knob_pos[2]:.2f}, v2: {self.knob_pos[3]:.2f}, ' + \\\n",
    "              f'injection efficiency: {self.inj_eff:.4f}')\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-metabolism",
   "metadata": {},
   "source": [
    "#### Check the BTS env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "unlikely-tsunami",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "stable-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1: 0.50, v1: 0.60, h5: 0.50, v2: 0.50, injection efficiency: 0.0000\n"
     ]
    }
   ],
   "source": [
    "env = BTSEnv(process, knob_step_size=0.1, max_steps=1000)\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True, skip_render_check=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-fetish",
   "metadata": {},
   "source": [
    "The following one is a manual version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "expressed-fisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "dX: 0.4797, 0.5616, 0.4959, 0.4169\n",
      "\n",
      "h1: 0.50, v1: 0.50, h5: 0.50, v2: 0.40, injection efficiency: 0.9080\n",
      "h1: 0.50, v1: 0.50, h5: 0.60, v2: 0.40, injection efficiency: 0.4130\n",
      "h1: 0.50, v1: 0.40, h5: 0.60, v2: 0.40, injection efficiency: 0.0000\n",
      "h1: 0.50, v1: 0.40, h5: 0.60, v2: 0.30, injection efficiency: 0.0000\n",
      "h1: 0.50, v1: 0.30, h5: 0.60, v2: 0.30, injection efficiency: 0.0000\n",
      "h1: 0.50, v1: 0.30, h5: 0.50, v2: 0.30, injection efficiency: 0.0000\n",
      "h1: 0.50, v1: 0.30, h5: 0.60, v2: 0.30, injection efficiency: 0.0000\n",
      "h1: 0.40, v1: 0.30, h5: 0.60, v2: 0.30, injection efficiency: 0.0000\n",
      "h1: 0.40, v1: 0.30, h5: 0.70, v2: 0.30, injection efficiency: 0.0000\n",
      "h1: 0.40, v1: 0.30, h5: 0.70, v2: 0.20, injection efficiency: 0.0000\n"
     ]
    }
   ],
   "source": [
    "env = BTSEnv(process, knob_step_size=0.1, max_steps=1000, verbose=True)\n",
    "obs = env.reset()\n",
    "n_steps = 10\n",
    "for _ in range(n_steps):\n",
    "    # Random action\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-monster",
   "metadata": {},
   "source": [
    "### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "alien-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from tqdm.auto import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-directive",
   "metadata": {},
   "source": [
    "#### Instantiate the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "advance-legend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the env\n",
    "env = BTSEnv(process, knob_step_size=0.1, max_steps=100)\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "minor-toronto",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = 'rl-log/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Wrap the environment\n",
    "env = Monitor(env, log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-damage",
   "metadata": {},
   "source": [
    "#### Evaluate the model before the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "saving-boxing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "center-model",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.374599999999999, 16.34639357901308)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-hurricane",
   "metadata": {},
   "source": [
    "#### Train the RL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "sudden-uzbekistan",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2699c5aac08>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 1000 steps\n",
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-feedback",
   "metadata": {},
   "source": [
    "#### Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "subject-discovery",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "injured-communications",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0887, 0.15930163213225407)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-bacon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "coastal-appeal",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Backlogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "starting-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes to evaluate it\n",
    "    :return: (float) Mean reward for the last num_episodes\n",
    "    \"\"\"\n",
    "    # This function will only work for a single Environment\n",
    "    env = model.get_env()\n",
    "    all_episode_rewards = []\n",
    "    for i in trange(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        obs = env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_rewards.append(reward[0])\n",
    "\n",
    "        all_episode_rewards.append(episode_rewards)\n",
    "\n",
    "    return all_episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "continued-bolivia",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a0e973f9c0497d8608a832f66eb2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "rewards = evaluate(model, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "applied-property",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.27,\n",
       "  0.241,\n",
       "  0.27,\n",
       "  0.241,\n",
       "  0.521,\n",
       "  0.422,\n",
       "  0.803,\n",
       "  0.803,\n",
       "  0.422,\n",
       "  0.331,\n",
       "  0.664,\n",
       "  0.664,\n",
       "  0.477,\n",
       "  0.283,\n",
       "  0.152,\n",
       "  0.202,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.152,\n",
       "  0.246,\n",
       "  0.17,\n",
       "  0.1,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.069,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.156,\n",
       "  0.0,\n",
       "  0.422,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.018,\n",
       "  0.0,\n",
       "  0.018,\n",
       "  0.0,\n",
       "  0.002,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.2,\n",
       "  0.414,\n",
       "  0.576,\n",
       "  0.697,\n",
       "  0.765,\n",
       "  0.697,\n",
       "  0.006,\n",
       "  0.007,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.354,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " [0.339, 0.389, 0.884],\n",
       " [0.885],\n",
       " [0.91],\n",
       " [0.555, 0.904],\n",
       " [0.881],\n",
       " [0.907],\n",
       " [0.181, 0.899],\n",
       " [0.908],\n",
       " [0.89]]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-hungarian",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "meaningful-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = b = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "occupied-conservative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "exotic-samba",
   "metadata": {},
   "outputs": [],
   "source": [
    "low = np.array([-100] * 18 + [0] * 4)\n",
    "high = np.array([100] * 18 + [1] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "regulation-intent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100,    0,    0,    0,    0]),\n",
       " array([100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
       "        100, 100, 100, 100, 100,   1,   1,   1,   1]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low, high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-franklin",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unlikely-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 0.5 * np.ones((1, 22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "variable-wings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.908     ,  0.04201704,  0.01295851,  0.11609385,  0.01590106,\n",
       "        0.16815135,  0.02435282,  0.10337124,  0.0113079 , -0.00438291,\n",
       "       -0.00856344, -0.01582888, -0.02946351, -0.0279356 , -0.01365623,\n",
       "       -0.02691261, -0.00738051, -0.02849029,  0.00383037])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automatic-strength",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((np.ones(3), np.zeros(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "broke-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, 4] = 0.5001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "narrative-participation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.905     ,  0.02054788,  0.01209201,  0.0740073 ,  0.00985236,\n",
       "       -0.02654498,  0.0073793 , -0.24051572, -0.00655828, -0.33538095,\n",
       "       -0.01576651, -0.12284825,  0.00647419,  0.10363903,  0.0116303 ,\n",
       "        0.0881582 ,  0.03076752,  0.08574401,  0.06793308])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "piano-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0, 6] = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "accredited-declaration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teeport: already linked\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.908     ,  0.04201704,  0.01295851,  0.11609385,  0.01590106,\n",
       "        0.16815135,  0.02435282,  0.10337124,  0.0113079 , -0.00438291,\n",
       "       -0.00856344, -0.01582888, -0.02946351, -0.0279356 , -0.01365623,\n",
       "       -0.02691261, -0.00738051, -0.02849029,  0.00383037])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "chief-story",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,  -1.42496657,  -0.0268286 ,  -2.75821768,\n",
       "        -0.34319369, -13.36452708,  -1.00140158, -23.94383618,\n",
       "        -1.06070593, -23.16685735,  -0.42571632,  -7.27960197,\n",
       "         2.25141362,   9.61266597,   1.64698367,   8.3937019 ,\n",
       "         2.57812122,   8.32116593,   4.44035147])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process(X + 0.01 * np.random.rand(1, 22))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-newspaper",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "detected-claim",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 25.7     |\n",
      "|    ep_rew_mean        | 25.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 314      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.679   |\n",
      "|    explained_variance | 0.0612   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 1.95     |\n",
      "|    value_loss         | 9.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 27.7     |\n",
      "|    ep_rew_mean        | 27.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.682   |\n",
      "|    explained_variance | -0.00555 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 1.49     |\n",
      "|    value_loss         | 7.38     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 29.2     |\n",
      "|    ep_rew_mean        | 29.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 340      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.69    |\n",
      "|    explained_variance | 0.0167   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 1.49     |\n",
      "|    value_loss         | 6.55     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 29.3     |\n",
      "|    ep_rew_mean        | 29.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 344      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.616   |\n",
      "|    explained_variance | -0.00207 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -13.1    |\n",
      "|    value_loss         | 399      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 30.6     |\n",
      "|    ep_rew_mean        | 30.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 345      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.648   |\n",
      "|    explained_variance | -0.0309  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.941    |\n",
      "|    value_loss         | 5.54     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 32.7     |\n",
      "|    ep_rew_mean        | 32.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 341      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.652   |\n",
      "|    explained_variance | 0.00385  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.983    |\n",
      "|    value_loss         | 4.92     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 35.6     |\n",
      "|    ep_rew_mean        | 35.6     |\n",
      "| time/                 |          |\n",
      "|    fps                | 343      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.617   |\n",
      "|    explained_variance | -0.00305 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.16     |\n",
      "|    value_loss         | 4.33     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 39.5     |\n",
      "|    ep_rew_mean        | 39.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 345      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.552   |\n",
      "|    explained_variance | 0.00362  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.35     |\n",
      "|    value_loss         | 3.79     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 43       |\n",
      "|    ep_rew_mean        | 43       |\n",
      "| time/                 |          |\n",
      "|    fps                | 344      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.602   |\n",
      "|    explained_variance | 0.000924 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 1.06     |\n",
      "|    value_loss         | 3.27     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 46.8     |\n",
      "|    ep_rew_mean        | 46.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 346      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.642   |\n",
      "|    explained_variance | 0.000397 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.798    |\n",
      "|    value_loss         | 2.81     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 51        |\n",
      "|    ep_rew_mean        | 51        |\n",
      "| time/                 |           |\n",
      "|    fps                | 347       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.598    |\n",
      "|    explained_variance | -0.000207 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 0.546     |\n",
      "|    value_loss         | 2.37      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 53.5     |\n",
      "|    ep_rew_mean        | 53.5     |\n",
      "| time/                 |          |\n",
      "|    fps                | 344      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.506   |\n",
      "|    explained_variance | 0.000149 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.884    |\n",
      "|    value_loss         | 1.97     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 57.2      |\n",
      "|    ep_rew_mean        | 57.2      |\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.573    |\n",
      "|    explained_variance | -0.000175 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 0.568     |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 61.7     |\n",
      "|    ep_rew_mean        | 61.7     |\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.549   |\n",
      "|    explained_variance | 0.000239 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.457    |\n",
      "|    value_loss         | 1.29     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 65.3      |\n",
      "|    ep_rew_mean        | 65.3      |\n",
      "| time/                 |           |\n",
      "|    fps                | 333       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.532    |\n",
      "|    explained_variance | -0.000138 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 0.576     |\n",
      "|    value_loss         | 1         |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 69.3     |\n",
      "|    ep_rew_mean        | 69.3     |\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.524   |\n",
      "|    explained_variance | 2.96e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.524    |\n",
      "|    value_loss         | 0.742    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 72.8      |\n",
      "|    ep_rew_mean        | 72.8      |\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.594    |\n",
      "|    explained_variance | -1.41e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.306     |\n",
      "|    value_loss         | 0.52      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 76.1     |\n",
      "|    ep_rew_mean        | 76.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.546   |\n",
      "|    explained_variance | 1.76e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.339    |\n",
      "|    value_loss         | 0.345    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 80.4     |\n",
      "|    ep_rew_mean        | 80.4     |\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.385   |\n",
      "|    explained_variance | 2.28e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.336    |\n",
      "|    value_loss         | 0.2      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 84.9     |\n",
      "|    ep_rew_mean        | 84.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.404   |\n",
      "|    explained_variance | -1.7e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 0.19     |\n",
      "|    value_loss         | 0.0947   |\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "model = A2C('MlpPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-sleeve",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
